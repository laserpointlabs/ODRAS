# GitHub Artifacts for DAS Testing

**Purpose**: Track DAS performance and test results across commits
**Location**: GitHub Actions â†’ Workflow runs â†’ Artifacts
**Retention**: 30-90 days depending on artifact type

## Overview

Every commit triggers comprehensive DAS testing with detailed reporting. Results are automatically uploaded as GitHub artifacts, providing:

âœ… **Historical tracking** of DAS performance
âœ… **Easy access** to test reports without digging through CI logs
âœ… **Trend analysis** data for performance optimization
âœ… **Debug information** when tests fail
âœ… **Comparison** across commits and features

---

## Available Artifacts

### **1. DAS Test Reports** (30-day retention)

**Artifact Name**: `das-test-reports-{commit-sha}`

**Contents**:
- **ğŸ“„ `das_test_report_YYYYMMDD_HHMMSS.md`**: Comprehensive markdown report
- **ğŸ” `*das*_report.json`**: Structured test results (pytest JSON format)
- **ğŸ“‹ Application logs**: `odras_app.log`, `odras_complex_worker.log`, `odras_simple_worker.log`

**Use Cases**:
- **Review test failures**: Download when CI fails to see detailed error analysis
- **Validate DAS enhancements**: Check before/after performance on feature branches
- **Debug context issues**: Examine logs when DAS gives unexpected answers

**Sample Report Structure**:
```markdown
ğŸ¯ DAS COMPREHENSIVE TEST REPORT
Generated: 2025-10-06 14:30:15

ğŸ“Š EXECUTIVE SUMMARY:
âœ¨ Overall Grade: A- (Excellent)
ğŸ“ˆ Success Rate: 95.2% (20/21 tests passed)
ğŸ§  Average Confidence: 78.3%

ğŸ¯ DAS CAPABILITIES ASSESSMENT:
âœ… Ontology Intelligence: EXCELLENT (Rich attributes, metadata, hierarchy)
âœ… Context Awareness: GOOD (Comprehensive understanding)
âœ… Overall System: RELIABLE

ğŸ’¡ RECOMMENDATIONS:
ğŸ‰ EXCELLENT: All critical tests passing!
ğŸ“ˆ Next Steps: Consider hybrid search implementation
```

---

### **2. DAS Performance Data** (90-day retention)

**Artifact Name**: `das-performance-data-{commit-sha}`

**Contents**:
- **â±ï¸ `das_performance_YYYYMMDD_HHMMSS.json`**: Timing and success metrics
- **ğŸ§  `das_confidence_YYYYMMDD_HHMMSS.json`**: LLM confidence scores
- **ğŸ“Š `das_summary_YYYYMMDD_HHMMSS.json`**: High-level summary for dashboards

**Use Cases**:
- **Performance regression detection**: Compare response times across commits
- **Confidence trend analysis**: Track DAS answer quality over time
- **Capacity planning**: Monitor test duration growth
- **Feature impact assessment**: Measure confidence changes after enhancements

**Sample Performance Data**:
```json
{
  "timestamp": "2025-10-06 14:30:15",
  "total_duration": 245.7,
  "suite_performance": {
    "ontology_attributes": {
      "duration_seconds": 181.2,
      "success_rate": 100.0,
      "avg_confidence": 85.4
    },
    "context_awareness": {
      "duration_seconds": 64.5,
      "success_rate": 88.9,
      "avg_confidence": 67.2
    }
  },
  "overall_metrics": {
    "total_tests": 15,
    "total_passed": 14,
    "overall_success_rate": 93.3
  }
}
```

---

## How to Access Artifacts

### **ğŸ“¥ Downloading from GitHub**

1. **Navigate to commit**: Go to your commit in GitHub
2. **Find workflow run**: Click on the green âœ… or red âŒ next to commit
3. **Select workflow**: Click on "CI" workflow run
4. **Download artifacts**: Scroll to bottom â†’ "Artifacts" section
5. **Extract files**: Download ZIP and extract locally

### **ğŸ”— Direct Links Pattern**

```
https://github.com/{owner}/{repo}/actions/runs/{run-id}
â†’ Scroll to "Artifacts" section
â†’ Click "das-test-reports-{sha}" or "das-performance-data-{sha}"
```

### **ğŸ“± GitHub CLI Access**

```bash
# List recent workflow runs
gh run list --workflow=ci.yml

# Download artifacts for specific run
gh run download {run-id} -n "das-test-reports-{sha}"
gh run download {run-id} -n "das-performance-data-{sha}"
```

---

## Using Artifacts for Analysis

### **ğŸ” Debugging Failed Tests**

**When CI fails**:
1. **Download**: `das-test-reports-{sha}.zip`
2. **Extract and review**: `das_test_report_*.md` for executive summary
3. **Check logs**: `odras_app.log` for backend errors
4. **JSON data**: `*_report.json` for detailed test metadata

**Common Failure Patterns**:
- **Service startup issues**: Check application logs
- **Authentication failures**: Look for 401/403 errors
- **Timeout issues**: Check performance data for slow suites
- **Context problems**: Review confidence scores for specific areas

### **ğŸ“ˆ Performance Trend Analysis**

**Track DAS improvement over time**:
1. **Collect data**: Download performance artifacts from multiple commits
2. **Extract metrics**: Parse `das_performance_*.json` files
3. **Create charts**: Plot confidence scores and response times over time
4. **Identify trends**: Look for improvements or regressions

**Key Metrics to Track**:
```python
# Sample analysis script
import json
import matplotlib.pyplot as plt

# Load multiple performance files
performance_files = ["das_performance_20251006_143000.json", ...]
dates = []
ontology_scores = []
response_times = []

for file in performance_files:
    with open(file) as f:
        data = json.load(f)
        dates.append(data["timestamp"])
        ontology_scores.append(data["suite_performance"]["ontology_attributes"]["avg_confidence"])
        response_times.append(data["suite_performance"]["ontology_attributes"]["duration_seconds"])

# Plot trends
plt.plot(dates, ontology_scores, label="Ontology Intelligence")
plt.plot(dates, response_times, label="Response Time (s)")
```

### **ğŸ¯ Feature Impact Assessment**

**Before/after feature development**:
1. **Baseline**: Download artifacts from commit before feature
2. **Enhanced**: Download artifacts from commit after feature
3. **Compare**: Side-by-side analysis of confidence scores and performance
4. **Validate**: Ensure feature improves target areas without regression

**Sample Comparison**:
```
Feature: Hybrid Search Implementation

Before (commit abc123):
  Ontology Confidence: 85.4%
  Technical Question Confidence: 45.2%
  Response Time: 12.3s

After (commit def456):
  Ontology Confidence: 86.1% (+0.7%)
  Technical Question Confidence: 67.8% (+22.6%) â† Significant improvement
  Response Time: 14.1s (+1.8s) â† Acceptable trade-off
```

---

## Artifact Contents Guide

### **ğŸ“„ Markdown Reports**

**Executive Summary**: Overall grade, success rate, key metrics
**Detailed Results**: Per-suite breakdown with failures
**Performance Analysis**: Timing analysis and optimization suggestions
**Context Coverage**: Assessment of all DAS context types
**Recommendations**: Specific action items based on results

**Best for**: Quick review, sharing with stakeholders, documentation

### **ğŸ“Š JSON Performance Data**

**Structured metrics** for programmatic analysis:
- Suite timing and success rates
- Overall test statistics
- Historical comparison data

**Best for**: Automated analysis, dashboard creation, trend tracking

### **ğŸ§  JSON Confidence Data**

**LLM evaluation scores** for answer quality:
- Individual question confidence scores
- Statistical summaries (min, max, average)
- Question-level performance breakdown

**Best for**: DAS intelligence analysis, answer quality trends, enhancement validation

### **ğŸ“‹ JSON Summary Data**

**High-level overview** for dashboards:
- Overall grades and capabilities assessment
- Suite-level pass/fail status
- Key performance indicators

**Best for**: Management dashboards, quick health checks, historical trends

---

## Automation Opportunities

### **ğŸ“Š Dashboard Integration**

**Connect to monitoring systems**:
```bash
# Sample webhook to process artifacts
curl -H "Authorization: token $GITHUB_TOKEN" \
     "https://api.github.com/repos/$REPO/actions/artifacts" \
| jq '.artifacts[] | select(.name | contains("das-performance"))'
```

### **ğŸ”” Alert Configuration**

**Set up alerts for regressions**:
```yaml
# .github/workflows/das-alerts.yml
- name: Check DAS Performance Regression
  run: |
    # Download latest performance data
    # Compare with baseline
    # Alert if significant regression detected
```

### **ğŸ“ˆ Trend Reports**

**Weekly/monthly DAS performance summaries**:
- Aggregate confidence scores across commits
- Identify performance trends
- Highlight successful enhancements
- Track capability maturity

---

## Best Practices

### **ğŸ” Regular Review Process**

**Weekly** (for active development):
1. **Check latest artifacts** from main branch
2. **Review confidence trends** for any degradation
3. **Monitor performance** for slow-down indicators
4. **Validate enhancements** show expected improvements

**Monthly** (for maintenance):
1. **Aggregate statistics** across all commits
2. **Identify patterns** in failures or performance issues
3. **Update test thresholds** based on sustained improvements
4. **Archive old artifacts** if storage becomes an issue

### **ğŸ¯ Using for Feature Development**

**Before starting enhancement**:
1. **Download baseline artifacts** from current main branch
2. **Note current confidence levels** for target area
3. **Set improvement goals** based on enhancement scope

**During development**:
1. **Run tests locally** using `scripts/generate_das_test_report.py`
2. **Compare with baseline** to track progress
3. **Adjust implementation** based on confidence score changes

**Before merge**:
1. **Download latest artifacts** from feature branch
2. **Validate improvement goals** achieved
3. **Check for regressions** in other areas
4. **Document changes** in PR with before/after metrics

---

## Sample Usage Scenarios

### **Scenario 1: CI Failure Investigation**

```bash
# 1. CI fails on commit abc123
# 2. Go to GitHub â†’ Actions â†’ Failed run
# 3. Download: das-test-reports-abc123.zip
# 4. Review: das_test_report_20251006_143000.md

Key findings from report:
- Context awareness test failed (timeout)
- Average confidence dropped from 67% to 23%
- Recommendation: Check ontology fetching performance

# 5. Review logs: odras_app.log shows SPARQL timeout errors
# 6. Fix: Optimize SPARQL queries, increase timeout
```

### **Scenario 2: Feature Enhancement Validation**

```bash
# Enhancement: Added hybrid search capability
# Goal: Improve technical question confidence from 45% to 65%

# Before (baseline commit):
Technical Question Confidence: 45.2%
Response Time: 12.3s

# After (feature commit):
Technical Question Confidence: 68.1% âœ… Goal exceeded
Response Time: 15.7s âš ï¸ Slight increase acceptable

# Validation: Feature successful, performance impact minimal
```

### **Scenario 3: Monthly DAS Health Review**

```bash
# Download artifacts from last 10 commits
# Extract confidence trends:

October Performance Summary:
- Ontology Intelligence: 85.4% avg (stable, excellent)
- Context Awareness: 56.8% avg (improving +5% from September)
- Response Times: 18.2s avg (stable)

# Recommendations:
- Continue ontology context enhancements âœ…
- Focus on conversation memory improvements ğŸ“ˆ
- Monitor performance with upcoming hybrid search ğŸ“Š
```

---

## Integration with Development Workflow

### **ğŸ”„ Pull Request Integration**

```markdown
<!-- PR Template Addition -->
## DAS Impact Assessment

**Baseline Performance** (commit abc123):
- Download: [das-performance-data-abc123](link-to-artifacts)
- Ontology Confidence: 85.4%
- Context Awareness: 56.8%
- Response Time: 18.2s

**Enhanced Performance** (this PR):
- Ontology Confidence: 89.1% (+3.7% improvement)
- Context Awareness: 61.2% (+4.4% improvement)
- Response Time: 19.1s (+0.9s acceptable)

**Validation**: âœ… Enhancement successful, no regressions detected
```

### **ğŸ¯ Release Planning**

**Use artifacts to plan releases**:
1. **Aggregate improvements** across all PRs since last release
2. **Identify major enhancements** with confidence score jumps
3. **Document DAS capabilities** for release notes
4. **Set targets** for next release cycle

---

## Conclusion

GitHub artifacts provide a **comprehensive historical record** of DAS test performance, enabling:

âœ… **Quality Assurance**: Automatic tracking of DAS capabilities
âœ… **Performance Monitoring**: Response time and confidence tracking
âœ… **Regression Detection**: Quick identification of issues
âœ… **Enhancement Validation**: Objective measurement of improvements
âœ… **Historical Analysis**: Trend identification and planning

**Next Steps**:
- Use artifacts to track hybrid search enhancement impact
- Create automated alerting for performance regressions
- Build dashboard for DAS capability maturity tracking

**Related Documentation**:
- `DAS_COMPREHENSIVE_TESTING_SUMMARY.md` - Complete testing strategy
- `MULTI_ENDPOINT_ONTOLOGY_ISSUE.md` - API guidance and deprecations
- `DAS2_PROMPT_GENERATION_ARCHITECTURE.md` - Enhancement roadmap

