name: ODRAS Complete System CI

on:
  push:
    branches: [main, develop, feature/*, fix/*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

jobs:
  complete_odras_test:
    name: Complete ODRAS System Test
    runs-on: ubuntu-latest
    timeout-minutes: 45

    env:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: odras
      NEO4J_AUTH: neo4j/testpassword
      FUSEKI_ADMIN_PASSWORD: admin
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    # No GitHub Actions services - use ./odras.sh up instead for exact local setup

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Set up Java 21
        uses: actions/setup-java@v4
        with:
          distribution: "temurin"
          java-version: "21"

      - name: Set up Docker Buildx (for caching)
        uses: docker/setup-buildx-action@v3

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            postgresql-client \
            redis-tools \
            wget \
            curl \
            net-tools \
            docker-compose

          # Install Neo4j cypher-shell (modern GPG approach)
          echo "Installing Neo4j cypher-shell with Java 21..."
          wget -O /tmp/neo4j.asc https://debian.neo4j.com/neotechnology.gpg.key
          sudo gpg --dearmor -o /usr/share/keyrings/neo4j.gpg /tmp/neo4j.asc
          echo 'deb [signed-by=/usr/share/keyrings/neo4j.gpg] https://debian.neo4j.com stable latest' | sudo tee /etc/apt/sources.list.d/neo4j.list
          sudo apt-get update
          sudo apt-get install -y cypher-shell

          # Verify installation
          echo "Java version:"
          java -version
          echo "cypher-shell version:"
          cypher-shell --version || echo "cypher-shell installation failed"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio psycopg2-binary

      - name: Start ODRAS stack (CI-compatible)
        run: |
          echo "üöÄ Starting ODRAS stack (CI-compatible with CPU ollama)..."
          # Start core services (no GPU profile)
          docker-compose up -d
          # Optionally start CPU ollama for LLM testing
          docker-compose -f docker-compose.yml -f docker-compose.ci.yml up -d ollama

          echo "Waiting for services to be ready..."
          sleep 20

          # Verify critical services (same as odras.sh)
          echo "Checking PostgreSQL..."
          for i in {1..30}; do
            if pg_isready -h localhost -p 5432 -U postgres > /dev/null 2>&1; then
              echo "‚úÖ PostgreSQL ready"
              break
            fi
            sleep 2
          done

          echo "Checking Redis..."
          for i in {1..30}; do
            if redis-cli -h localhost -p 6379 ping > /dev/null 2>&1; then
              echo "‚úÖ Redis ready"
              break
            fi
            sleep 2
          done

          echo "‚úÖ ODRAS stack ready with proper container names and GPU fallback"

      - name: Initialize database (EXACTLY like local)
        run: |
          echo "üóÑÔ∏è Running ./odras.sh init-db (SAME AS LOCAL)..."
          ./odras.sh init-db

      - name: Start ODRAS application (EXACTLY like local)
        run: |
          echo "üöÄ Starting ODRAS application using ./odras.sh start (SAME AS LOCAL)..."
          ./odras.sh start

          echo "Waiting for application startup..."
          sleep 30

          # Verify COMPLETE system
          echo "üîç Verifying COMPLETE ODRAS system..."

          # API Health
          for i in {1..30}; do
            if curl -s http://localhost:8000/api/health > /dev/null; then
              health_data=$(curl -s http://localhost:8000/api/health)
              echo "‚úÖ ODRAS API responding"
              echo "üìä Health status: $health_data"
              break
            fi
            echo "API not ready... ($i/30)"
            sleep 3
          done

          # Service status
          service_status=$(curl -s http://localhost:8000/api/service-status || echo "Service status not available")
          echo "üìä Service status: $service_status"

          echo "üéØ COMPLETE ODRAS APPLICATION READY"

      - name: Run COMPLETE ODRAS system tests (IDENTICAL to local)
        run: |
          echo "üéØ Running COMPLETE ODRAS system tests (EXACT SAME AS LOCAL)"
          echo "============================================================"

          echo "üèÉ‚Äç‚ôÇÔ∏è Step 1: Fast DAS Health Check..."
          python scripts/fast_das_validator.py

          echo "üß™ Step 2: Enhanced Ontology Attributes Test..."
          python -m pytest tests/test_working_ontology_attributes.py -v --tb=short

          echo "üß™ Step 3: Baseline DAS Integration Test..."
          python tests/test_das_integration_comprehensive.py

          echo "üß™ Step 4: RAG System Stability Test..."
          python tests/test_rag_system_stability.py

          echo "‚úÖ COMPLETE ODRAS system validation finished"
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: COMPLETE system diagnostic report
        if: always()
        run: |
          echo "üìä COMPLETE ODRAS SYSTEM DIAGNOSTIC REPORT"
          echo "============================================================"

          echo "DATABASE TABLES:"
          PGPASSWORD=password psql -h localhost -U postgres -d odras -c "
          SELECT table_name FROM information_schema.tables
          WHERE table_schema = 'public'
          ORDER BY table_name;" || echo "PostgreSQL unavailable"

          echo "TEST USERS:"
          PGPASSWORD=password psql -h localhost -U postgres -d odras -c "
          SELECT username, is_admin, is_active FROM users ORDER BY username;" || echo "Users check failed"

          echo "QDRANT COLLECTIONS:"
          curl -s http://localhost:6333/collections | python -c "
          import json, sys
          try:
              data = json.load(sys.stdin)
              collections = data.get('result', {}).get('collections', [])
              print(f'Total collections: {len(collections)}')
              for c in collections:
                  print(f'  ‚úÖ {c}')
          except Exception as e:
              print(f'Qdrant check failed: {e}')
          " || echo "Qdrant unavailable"

          echo "NEO4J STATUS:"
          cypher-shell -a neo4j://localhost:7687 -u neo4j -p testpassword "
          CALL db.labels() YIELD label RETURN label LIMIT 10;
          " || echo "Neo4j unavailable"

          echo "REDIS STATUS:"
          redis-cli -h localhost -p 6379 info server | head -5 || echo "Redis unavailable"

          echo "FUSEKI STATUS:"
          curl -s http://localhost:3030/$/ping || echo "Fuseki unavailable"

          echo "CAMUNDA STATUS:"
          curl -s http://localhost:8080/engine-rest/version || echo "Camunda unavailable"

          echo "DEPLOYED PROCESS DEFINITIONS:"
          curl -s http://localhost:8080/engine-rest/process-definition | python -c "
          import json, sys
          try:
              data = json.load(sys.stdin)
              print(f'Process definitions: {len(data)}')
              for proc in data[:5]:
                  print(f'  ‚úÖ {proc.get(\"key\", \"unknown\")} - {proc.get(\"name\", \"no name\")}')
          except:
              print('No process definitions or Camunda unavailable')
          " || echo "Process definition check failed"

          echo "API LOGS (last 50 lines):"
          tail -50 /tmp/odras_api.log || echo "No API logs"

          echo "WORKER LOGS (last 20 lines):"
          tail -20 /tmp/odras_worker.log || echo "No worker logs"

          echo "============================================================"
          echo "üéâ ALL TESTS COMPLETED IN MAIN SECTION"
          echo "üìä System diagnostic and logs captured"
          echo "============================================================"

      - name: Upload DAS Test Reports
        uses: actions/upload-artifact@v4
        if: always() # Upload artifacts even if tests fail
        with:
          name: das-test-reports-${{ github.sha }}
          path: |
            /tmp/das_integration_test_*.log
            /tmp/odras_app.log
            /tmp/odras_complex_worker.log
            /tmp/odras_simple_worker.log
            /tmp/rag_test_*.log
          retention-days: 30

      - name: Upload DAS Performance Data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: das-performance-data-${{ github.sha }}
          path: |
            /tmp/das_simple_metrics_*.json
            /tmp/*test_results*.json
            /tmp/*confidence*.json
            /tmp/*performance*.json
          retention-days: 90 # Keep performance data longer for trend analysis

  # REMOVED: das-integration-test job to eliminate port conflicts
  # DAS test now runs within main job using shared infrastructure
